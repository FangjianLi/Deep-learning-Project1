# Deep-learning-Project 1

The codes are based on tensorflow 1.14. Python 3.6.  They can be run in the jupyter-notebook.

## Deep a function
### Simulate a function: proj1-1 Simulated function_DNN.ipynb
### Train on actual task: proj1-1 MINST_DNN.ipynb
### CNN is used to train the task as well: proj1-1 MINST_CNN.ipynb

## Optimization
### Visualize the optimization process: proj1-2 Weights PCA.ipynb
### Observe gradient norm during training: porj1-2 Optimization_gradient_function.ipynb
### What happens when gradient is almost zero: proj1-2 Optimization_gradient_function.ipynb

## Generalizaton
### Can network fit random labels?: proj1-3_MINST_DNN_Shuffled_label.ipynb
### Number of parameters v.s. Generalization: proj1-3_MINST_DNN_Ten_Models.ipynb
### Flatness v.s. Generalization - part1: proj1-3_MINST_DNN_different batch.ipynb
### Flatness v.s. Generalization - part2: proj1-3_MINST_DNN_sensitivity.ipynb

